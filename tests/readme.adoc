== Functional tests ==

See link:https://hurl.dev/[hurl.dev] for upstream documentation.


.Run local tests
----
# The API must be running in another terminal
make tokens
. .dev.tokens_env
uuid=$(uuidgen -r)

cd tests/

hurl --test \
  --variable login_token=$apptoken \
  --variable login_token_admin=$admintoken \
  --variable host=http://localhost:8080 \
  --variable uuid=$uuid \
  --jobs 1 \
  *.hurl
----

.Example output
----
000.hurl: Running [1/2]
000.hurl: Success (36 request(s) in 35304 ms)
999.hurl: Running [2/2]
999.hurl: Success (2 request(s) in 371 ms)
--------------------------------------------------------------------------------
Executed files:  2
Succeeded files: 2 (100.0%)
Failed files:    0 (0.0%)
Duration:        35680 ms
----

== OCP tests ==

=== requirements ===

* ocp cluster with matching annotations, see below
* the entry in the `ocp_shared_cluster_configurations` table

=== Create the OcpSharedClusterConfiguration entry ===

Either you're using the DEV database,  or you'll have to setup the OCP shared cluster configuration in your local DB table `ocp_cluster`. That can be done using link:../tools/ocp_shared_cluster_configuration_create.hurl[ocp_shared_cluster_configuration_create.hurl]


[source,json]
.`.dev.kube.json`
----
{
  "name": "cluster-foo",
  "api_url": "https://api.domain.com:6443",
  "annotations":
    {
      "virt":"no",
      "cloud":"cnv",
      "name": "cluster-foo",
      "purpose":"dev"
    },
  "kubeconfig": "..."
}
----

The tricky part is to embed the kubeconfig, that is usually in the YAML format into a JSON string. This can be done with the following command:

----
jq --raw-input --slurp < KUBECONFIGFILE
----

For example if you have access to an ocp-cluster bastion, you can do:
----
ssh user@bastion cat .kube/config|jq --raw-input --slurp
----

.Add the entry to the DB
----
hurl --variable login_token_admin=$admintoken \
--file-root . \
--variable host=http://localhost:8080 \
--variable ocp_cluster_def=.dev.kube.json \
./tools/ocp_shared_cluster_configuration_create.hurl


# or with curl directly
curl --header "Authorization: Bearer $admintoken" \
  --header 'Content-Type: application/json' \
  --data-binary '@./.dev.kube.json' \
  'http://localhost:8080/api/v1/ocp-shared-cluster-configurations'
----

.Run the tests for OcpSandbox
----
uuid=$(uuidgen -r)
cd tests

hurl --variable login_token_admin=$admintoken \
--variable login_token=$apptoken \
--variable host=http://localhost:8080 \
--variable uuid=$uuid \
002_ocp.hurl --test
----

=== Troubleshoot ===

Add the `--verbose` argument to the `hurl` command to see the full requests.

== Jenkins CI Tests ==

The `jenkins-run.sh` script runs comprehensive tests including:

1. **Hurl API tests** - All `*.hurl` tests against the sandbox-api
2. **sandbox-ctl CLI functional tests** - Tests the CLI binary directly against a real OCP cluster
3. **Ansible role tests** - Tests the `sandbox_ctl` Ansible role example playbooks

=== Running Jenkins Tests Locally ===

The jenkins-run.sh script requires:

* **Bitwarden CLI access** - For fetching cluster credentials
* **OCP cluster configurations** - In `sandbox-api-configs/ocp-shared-cluster-configurations/`
* **Environment variables**:
  - `BWS_ACCESS_TOKEN` - Bitwarden Secrets Manager access token
  - `BWS_PROJECT_ID` - Bitwarden project ID

.Run all tests (including sandbox-ctl and Ansible role tests)
----
cd tests
BWS_ACCESS_TOKEN=your-token BWS_PROJECT_ID=your-project ./jenkins-run.sh
----

The script will:

1. Download `oc` binary if not available (cached in `~/.local/bin`)
2. Start PostgreSQL database for API tests
3. Run database migrations
4. Start sandbox-api
5. Run hurl API tests
6. Build sandbox-ctl binary
7. Get credentials for ocpvdev01 cluster from Bitwarden
8. Run sandbox-ctl functional tests (test-sandbox-ctl.sh)
9. Install ansible-core and kubernetes.core collection
10. Run sandbox_ctl Ansible role example playbooks:
    - single-sandbox.yml
    - multiple-sandboxes.yml
    - with-other-role.yml

=== OC Binary Management ===

The `oc` binary is automatically downloaded if not available:

* **Download source**: https://mirror.openshift.com/pub/openshift-v4/clients/oc/latest/linux/oc.tar.gz
* **Install location**: `~/.local/bin/oc`
* **Cached**: Downloaded once and reused across test runs

To manually install `oc`:
----
mkdir -p ~/.local/bin
curl -sL https://mirror.openshift.com/pub/openshift-v4/clients/oc/latest/linux/oc.tar.gz | tar -xz -C ~/.local/bin oc
chmod +x ~/.local/bin/oc
export PATH="$HOME/.local/bin:$PATH"
----

=== Cluster Credentials from Bitwarden ===

The script fetches cluster credentials from Bitwarden Secrets Manager:

----
# Cluster token stored as: {cluster-name}.token
# Example: ocpvdev01.token

CLUSTER_TOKEN=$(podman run --rm \
    -e BWS_ACCESS_TOKEN=$BWS_ACCESS_TOKEN \
    -e PROJECT_ID=$BWS_PROJECT_ID \
    --security-opt label=disable \
    --userns=host \
    bitwarden/bws:0.5.0 secret list $BWS_PROJECT_ID \
    | jq -r '.[] | select(.key=="ocpvdev01.token") | .value')
----
